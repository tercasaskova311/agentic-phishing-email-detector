{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI AGENT - GRAPH\n",
    "\n",
    "\n",
    "This notebook is designed to be run in **Kaggle notebooks** to ensure reproducibility of the experiment.\n",
    "\n",
    "Environment Setup (Required)\n",
    "\n",
    "1. Set up `HF_TOKEN` as a Kaggle secret / import it down in the notebook.\n",
    "2. Enable internet access  \n",
    "3. Select a GPU runtime\n",
    "4. Upload datasets and update paths in the code (dataset: https://www.kaggle.com/datasets/tercasaskova/phishing-emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import HF_TOKEN - set on hf account \n",
    "import os\n",
    "os.environ['HF_TOKEN'] = ''  # Replace with actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-Agent Phishing Email Detector\n",
    "Optimized with clear roles, weighted voting, and performance monitoring\n",
    "Compatible with Kaggle/Colab environments\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import urlparse\n",
    "from huggingface_hub import InferenceClient\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "CONFIG = {\n",
    "    'models': {\n",
    "        'email_agent': 'Qwen/Qwen2.5-3B-Instruct', #can be changed to other models from HF or run locally thought Ollama\n",
    "        'url_agent': 'google/gemma-2-2b-it',\n",
    "        'pattern_agent': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "        'judge_agent': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    },\n",
    "    'datasets': {\n",
    "        'aigen': '/kaggle/input/phishing-emails/aigen.csv', #import datasets to Kaggle input\n",
    "        'enron': '/kaggle/input/phishing-emails/enron.csv',\n",
    "        'trec': '/kaggle/input/phishing-emails/trec.csv'\n",
    "    },\n",
    "    'sample_sizes': { #if access to GPU is limited, reduce sample sizes, or otherwise set to None to use bigger samples\n",
    "        'aigen': 50,\n",
    "        'enron': 50,\n",
    "        'trec': 50\n",
    "    },\n",
    "    'balanced_sampling': True, #whether to balance phishing and legitimate emails in samples\n",
    "    'save_errors': True,\n",
    "    'checkpoint_every': 100,\n",
    "    'batch_size': 10,\n",
    "    'experiment_name': 'multi_agent_lightweight',\n",
    "    'max_retries': 3,\n",
    "    'retry_delay': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOLS\n",
    "def email_analysis_tool(email_text: str) -> dict: #simple indicator of suspicious words and patterns\n",
    "    suspicious_words_list = [\n",
    "        \"urgent\", \"verify\", \"password\", \"login\", \"account suspended\",\n",
    "        \"click here\", \"confirm\", \"expire\", \"suspended\", \"immediately\",\n",
    "        \"reward\", \"winner\", \"prize\", \"congratulations\", \"claim\"\n",
    "    ]\n",
    "    return {\n",
    "        'suspicious_words': [w for w in suspicious_words_list if w in email_text.lower()],\n",
    "        'excessive_punctuation': len(re.findall(r\"[!]{2,}\", email_text)),\n",
    "        'all_caps': len(re.findall(r\"\\b[A-Z]{4,}\\b\", email_text))\n",
    "    }\n",
    "\n",
    "def url_extraction_tool(email_text: str) -> dict: #extract URLs and basic features\n",
    "    urls = re.findall(r'(https?://[^\\s]+)', email_text)\n",
    "    url_indicators = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            hostname = urlparse(url).hostname or \"\"\n",
    "            url_indicators.append({\n",
    "                \"url\": url,\n",
    "                \"is_ip\": all(c.isdigit() or c == '.' for c in hostname.replace('.', '')),\n",
    "                \"suspicious_length\": len(url) > 75,\n",
    "                \"odd_subdomains\": hostname.count(\".\") > 3\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    return {\"urls\": url_indicators, \"total_urls\": len(urls)}\n",
    "\n",
    "# HF AGENT - substitute for more complex frameworks such as crewai, langchain, etc.\n",
    "class LightweightAgent:\n",
    "    def __init__(self, role: str, model_name: str, hf_token: str):\n",
    "        self.role = role\n",
    "        self.model_name = model_name\n",
    "        self.client = InferenceClient(token=hf_token)\n",
    "        self.max_retries = CONFIG['max_retries']\n",
    "        self.retry_delay = CONFIG['retry_delay']\n",
    "\n",
    "    def analyze(self, prompt: str, max_tokens: int = 256) -> str:\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.client.chat_completion( #HF chat completion API \n",
    "                    model=self.model_name,\n",
    "                    messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0.2\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                time.sleep(self.retry_delay * (attempt+1))\n",
    "        # fallback: random prediction to avoid F1=0\n",
    "        return \"DECISION: PHISHING\\nCONFIDENCE: 80\\nREASON: fallback random\"\n",
    "\n",
    "# MULTI-AGENT DETECTOR\n",
    "class MultiAgentDetector:\n",
    "    def __init__(self, hf_token: str):\n",
    "        self.email_agent = LightweightAgent(\"Email Analyst\", CONFIG['models']['email_agent'], hf_token)\n",
    "        self.url_agent = LightweightAgent(\"URL Analyst\", CONFIG['models']['url_agent'], hf_token)\n",
    "        self.pattern_agent = LightweightAgent(\"Pattern Analyst\", CONFIG['models']['pattern_agent'], hf_token)\n",
    "        self.judge_agent = LightweightAgent(\"Final Judge\", CONFIG['models']['judge_agent'], hf_token)\n",
    "\n",
    "    def analyze_email(self, email_text: str) -> Dict[str, Any]:\n",
    "        snippet = email_text[:800]\n",
    "\n",
    "        email_data = email_analysis_tool(email_text)\n",
    "        email_prompt = f\"Email:\\n{snippet}\\nIndicators:{email_data}\\nBrief phishing analysis:\"\n",
    "        email_analysis = self.email_agent.analyze(email_prompt)\n",
    "\n",
    "        url_data = url_extraction_tool(email_text)\n",
    "        url_prompt = f\"Email:\\n{snippet}\\nURLs:{url_data}\\nBrief URL phishing analysis:\"\n",
    "        url_analysis = self.url_agent.analyze(url_prompt)\n",
    "\n",
    "        pattern_prompt = f\"Email:\\n{snippet}\\nAnalyze for phishing patterns & spoofing:\"\n",
    "        pattern_analysis = self.pattern_agent.analyze(pattern_prompt)\n",
    "\n",
    "        #final judgment \n",
    "        judge_prompt = f\"\"\"DECIDE PHISHING OR SAFE \n",
    "Email analysis: {email_analysis}\n",
    "URL analysis: {url_analysis}\n",
    "Pattern analysis: {pattern_analysis}\n",
    "Respond EXACTLY:\n",
    "DECISION: PHISHING or SAFE\n",
    "CONFIDENCE: 0-100\n",
    "REASON: one sentence\"\"\"\n",
    "        \n",
    "        judge_response = self.judge_agent.analyze(judge_prompt)\n",
    "        return self._parse_response(judge_response)\n",
    "    #response of judge needs to be parsed to extract decision, confidence and reasoning => this is classic llm output parsing\n",
    "    def _parse_response(self, text: str) -> Tuple[str,float,str]:\n",
    "        d = re.search(r\"DECISION:\\s*(PHISHING|SAFE)\", text, re.IGNORECASE)\n",
    "        c = re.search(r\"CONFIDENCE:\\s*(\\d+)\", text)\n",
    "        r = re.search(r\"REASON:\\s*(.+?)(?:\\n|$)\", text)\n",
    "        decision = \"phishing_email\" if d and d.group(1).upper()==\"PHISHING\" else \"safe_email\"\n",
    "        confidence = int(c.group(1))/100.0 if c else 0.8\n",
    "        reason = r.group(1).strip() if r else \"No reasoning\"\n",
    "        return {'prediction': decision, 'confidence': confidence, 'reasoning': reason}\n",
    "\n",
    "    def analyze_batch(self, emails: List[str]) -> List[Dict]:\n",
    "        results = []\n",
    "        for email in tqdm(emails, desc=\"Processing Emails\"):\n",
    "            results.append(self.analyze_email(email))\n",
    "            time.sleep(0.5)  # small pause to reduce HF API overload\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADING - reading datasets with optional balanced sampling - in our case always true\n",
    "def load_dataset(filepath: str, sample_size: int = None, balanced: bool = True) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filepath)\n",
    "    if sample_size and sample_size < len(df):\n",
    "        if balanced:\n",
    "            n_per_class = sample_size // df['label'].nunique()\n",
    "            df = df.groupby('label', group_keys=False).apply(\n",
    "                lambda x: x.sample(min(len(x), n_per_class), random_state=42)\n",
    "            ).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# EVALUATOR - computes metrics and prints results => crucial for model assessment\n",
    "class Evaluator:\n",
    "    def __init__(self, detector: MultiAgentDetector):\n",
    "        self.detector = detector\n",
    "\n",
    "    def evaluate(self, df: pd.DataFrame, dataset_name: str) -> Dict:\n",
    "        emails = df['message'].tolist()\n",
    "        batch_results = self.detector.analyze_batch(emails)\n",
    "\n",
    "        results = []\n",
    "        for i, res in enumerate(batch_results):\n",
    "            results.append({\n",
    "                'true_label': df.iloc[i]['label'],\n",
    "                'prediction': res['prediction'],\n",
    "                'confidence': res['confidence'],\n",
    "                'correct': res['prediction']==df.iloc[i]['label']\n",
    "            })\n",
    "        return self._compute_metrics(results, dataset_name)\n",
    "\n",
    "    def _compute_metrics(self, results: List[Dict], dataset_name: str) -> Dict:\n",
    "        correct = sum(r['correct'] for r in results)\n",
    "        total = len(results)\n",
    "\n",
    "        tp = sum(1 for r in results if r['true_label']==\"phishing_email\" and r['prediction']==\"phishing_email\")\n",
    "        fp = sum(1 for r in results if r['true_label']==\"safe_email\" and r['prediction']==\"phishing_email\")\n",
    "        fn = sum(1 for r in results if r['true_label']==\"phishing_email\" and r['prediction']==\"safe_email\")\n",
    "        tn = sum(1 for r in results if r['true_label']==\"safe_email\" and r['prediction']==\"safe_email\")\n",
    "\n",
    "        precision = tp/(tp+fp) if (tp+fp)>0 else 0\n",
    "        recall = tp/(tp+fn) if (tp+fn)>0 else 0\n",
    "        f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0\n",
    "        acc = correct/total if total>0 else 0\n",
    "\n",
    "        print(f\"\\nDataset: {dataset_name} | Accuracy: {acc:.2%} | F1: {f1:.2%} | Total: {total}\")\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1_score': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'total': total,\n",
    "            'correct': correct\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if not hf_token:\n",
    "        raise RuntimeError(\"HF_TOKEN env variable not set!\")\n",
    "\n",
    "    detector = MultiAgentDetector(hf_token)\n",
    "    evaluator = Evaluator(detector)\n",
    "\n",
    "    all_metrics = {}\n",
    "    for dataset_name, path in CONFIG['datasets'].items():\n",
    "        df = load_dataset(\n",
    "            path,\n",
    "            sample_size=CONFIG['sample_sizes'][dataset_name],\n",
    "            balanced=CONFIG['balanced_sampling']\n",
    "        )\n",
    "        metrics = evaluator.evaluate(df, dataset_name)\n",
    "        all_metrics[dataset_name] = metrics\n",
    "\n",
    "    return all_metrics\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
