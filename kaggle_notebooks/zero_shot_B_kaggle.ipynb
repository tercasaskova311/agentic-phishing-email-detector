{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PHISHING DETECTOR - SINGLE AGENT - A/B testing - A\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#CONFIGURATION ===========================\n",
    "\n",
    "ZERO_SHOT_B = \"\"\"You are a cybersecurity analyst. Classify this email, decide if its a phishing email or normal email. Phishig email means its a spam.\n",
    "\n",
    "Email:\n",
    "{email_text}\n",
    "\n",
    "Is this phishing? Reply: PHISHING or SAFE only.\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "CONFIG = {\n",
    "    'model_name': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    #meta-llama/Llama-3.2-3B-Instruct',      # Needs HF token\n",
    "    #google/gemma-2-2b-it',                   # No token needed - but hf approval of conditions\n",
    "    #Qwen/Qwen2.5-3B-Instruct\n",
    "    #Qwen/Qwen2.5-7B-Instruct\n",
    "\n",
    "    'datasets': {\n",
    "        'aigen': '/kaggle/input/phishing-emails/aigen.csv',\n",
    "        'enron': '/kaggle/input/phishing-emails/enron.csv',\n",
    "        'trec': '/kaggle/input/phishing-emails/trec.csv'\n",
    "    },\n",
    "    \n",
    "    'sample_sizes': {\n",
    "        'aigen': None,\n",
    "        'enron': 3000,\n",
    "        'trec': 3000\n",
    "    },\n",
    "    \n",
    "    'generation': {\n",
    "        'max_new_tokens': 50,\n",
    "        'temperature': 0.1,\n",
    "        'do_sample': True,\n",
    "        'top_p': 0.9\n",
    "    },\n",
    "    \n",
    "    'balanced_sampling': True,\n",
    "    'save_errors': True,\n",
    "    'checkpoint_every': 500,  # More frequent checkpoints\n",
    "    'experiment_name': 'zero_shot_baseline',\n",
    "    'strategy': 'zero-shot',\n",
    "    'prompt_template': ZERO_SHOT_B,\n",
    "    'use_4bit': False,\n",
    "    'batch_size': 16,  # Increased for better GPU utilization\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# GPU SETUP\n",
    "# ============================================================================\n",
    "\n",
    "def setup_kaggle_gpu():\n",
    "    \"\"\"Configure GPU and authenticate with HuggingFace\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"KAGGLE GPU SETUP\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"✓ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        free_mem = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n",
    "        print(f\"✓ Free GPU Memory: {free_mem / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"⚠️  WARNING: No GPU detected!\")\n",
    "        raise RuntimeError(\"GPU not available! Please enable GPU in Kaggle settings.\")\n",
    "    \n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "        \n",
    "        from huggingface_hub import login\n",
    "        login(token=hf_token)\n",
    "        print(\"✓ HuggingFace authentication successful\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  HuggingFace token not found: {e}\")\n",
    "        print(\"   Continuing with open models only\")\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CLASS \n",
    "# ============================================================================\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "\n",
    "    def __init__(self, emails: List[str], labels: List[str], prompt_template: str):\n",
    "        self.emails = emails\n",
    "        self.labels = labels\n",
    "        self.prompt_template = prompt_template\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.emails)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        email_text = self.emails[idx][:800]  # Truncate long emails\n",
    "        prompt = self.prompt_template.format(email_text=email_text)\n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'label': self.labels[idx],\n",
    "            'idx': idx\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADER\n",
    "# ============================================================================\n",
    "\n",
    "class ModelLoader:\n",
    "    \"\"\"Handles model loading with Kaggle GPU optimizations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(model_name: str, use_4bit: bool = False):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"LOADING MODEL: {model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"left\"   # for encoderonly models => Llama, GPT... otherwise right padding is setup as default....\n",
    "        )\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model_kwargs = {\n",
    "            'trust_remote_code': True,\n",
    "            'low_cpu_mem_usage': True,\n",
    "        }\n",
    "        \n",
    "        if use_4bit:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            model_kwargs['quantization_config'] = quantization_config\n",
    "            model_kwargs['device_map'] = 'auto'\n",
    "            print(\"✓ Using 4-bit quantization\")\n",
    "        else:\n",
    "            model_kwargs['torch_dtype'] = torch.float16\n",
    "            model_kwargs['device_map'] = 'auto'\n",
    "            print(\"✓ Using FP16 (half precision)\")\n",
    "        \n",
    "        print(\"Loading model to GPU...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        \n",
    "        print(\"Creating inference pipeline...\")\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device_map='auto',\n",
    "            batch_size=CONFIG['batch_size']  # Set default batch size\n",
    "        )\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "            reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "            print(f\"\\n✓ Model loaded in {load_time:.1f}s\")\n",
    "            print(f\"✓ GPU Memory Allocated: {allocated:.2f} GB\")\n",
    "            print(f\"✓ GPU Memory Reserved: {reserved:.2f} GB\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return generator, tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# PHISHING DETECTOR - SIMPLIFIED & FIXED\n",
    "# ============================================================================\n",
    "\n",
    "class PhishingDetector:\n",
    "    def __init__(self, generator, tokenizer, config: Dict = CONFIG):\n",
    "        self.generator = generator\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.strategy = config['strategy']\n",
    "        self.prompt_template = config['prompt_template']\n",
    "\n",
    "    def analyze_batch(self, prompts: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        FIXED: Single-level batching for efficient GPU usage\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Single pipeline call with all prompts\n",
    "            outputs = self.generator(\n",
    "                prompts,\n",
    "                max_new_tokens=self.config['generation']['max_new_tokens'],\n",
    "                temperature=self.config['generation']['temperature'],\n",
    "                do_sample=self.config['generation']['do_sample'],\n",
    "                top_p=self.config['generation']['top_p'],\n",
    "                return_full_text=False,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "            # Parse results\n",
    "            results = []\n",
    "            for output in outputs:\n",
    "                response = output[0]['generated_text'].strip()\n",
    "                result = self._parse_simple_response(response)\n",
    "                results.append(result)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  Batch processing error: {e}\")\n",
    "            # Fallback: process one at a time\n",
    "            results = []\n",
    "            for prompt in prompts:\n",
    "                try:\n",
    "                    output = self.generator(\n",
    "                        prompt,\n",
    "                        max_new_tokens=self.config['generation']['max_new_tokens'],\n",
    "                        temperature=self.config['generation']['temperature'],\n",
    "                        do_sample=self.config['generation']['do_sample'],\n",
    "                        top_p=self.config['generation']['top_p'],\n",
    "                        return_full_text=False,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "                    response = output[0]['generated_text'].strip()\n",
    "                    result = self._parse_simple_response(response)\n",
    "                    results.append(result)\n",
    "                except Exception as e2:\n",
    "                    results.append(('error', f'Error: {str(e2)[:100]}'))\n",
    "            \n",
    "            return results\n",
    "    \n",
    "    def _parse_simple_response(self, response: str) -> Tuple[str, str]:\n",
    "        response_upper = response.upper()\n",
    "        if 'PHISHING' in response_upper:\n",
    "            return 'phishing_email', response[:100]\n",
    "        elif 'SAFE' in response_upper:\n",
    "            return 'safe_email', response[:100]\n",
    "        else:\n",
    "            return 'error', f'Unclear: {response[:100]}'\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_dataset(\n",
    "    filepath: str,\n",
    "    sample_size: Optional[int] = None,\n",
    "    balanced: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    print(f\"\\nLoading: {filepath}\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"  Original size: {len(df):,}\")\n",
    "    \n",
    "    class_counts = df['label'].value_counts()\n",
    "    for label, count in class_counts.items():\n",
    "        print(f\"    {label}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if sample_size and sample_size < len(df):\n",
    "        if balanced:\n",
    "            n_per_class = sample_size // len(class_counts)\n",
    "            df = df.groupby('label', group_keys=False).apply(\n",
    "                lambda x: x.sample(min(len(x), n_per_class), random_state=42)\n",
    "            ).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Sampled size: {len(df):,}\")\n",
    "        sampled_counts = df['label'].value_counts()\n",
    "        print(f\"  Sampled distribution:\")\n",
    "        for label, count in sampled_counts.items():\n",
    "            print(f\"    {label}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATOR - FIXED BATCHING\n",
    "# ============================================================================\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, detector: PhishingDetector, checkpoint_dir: Path = Path('.')):\n",
    "        self.detector = detector\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "    \n",
    "    def evaluate(self, df: pd.DataFrame, dataset_name: str, \n",
    "                 checkpoint_every: int = 500, batch_size: int = 16) -> Dict:\n",
    "        \"\"\"\n",
    "        FIXED: Proper batching without nested loops\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EVALUATING: {dataset_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total emails: {len(df):,}\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        \n",
    "        results = []\n",
    "        errors = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare all prompts at once\n",
    "        prompts = [\n",
    "            self.detector.prompt_template.format(email_text=text[:800])\n",
    "            for text in df['message']\n",
    "        ]\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, len(df), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(df))\n",
    "            batch_prompts = prompts[batch_start:batch_end]\n",
    "            batch_df = df.iloc[batch_start:batch_end]\n",
    "            \n",
    "            # Get predictions for this batch\n",
    "            batch_predictions = self.detector.analyze_batch(batch_prompts)\n",
    "            \n",
    "            # Store results\n",
    "            for idx, (prediction, response) in enumerate(batch_predictions):\n",
    "                actual_idx = batch_start + idx\n",
    "                row = batch_df.iloc[idx]\n",
    "                \n",
    "                result = {\n",
    "                    'email_id': actual_idx,\n",
    "                    'true_label': row['label'],\n",
    "                    'prediction': prediction,\n",
    "                    'response': response[:200],\n",
    "                    'correct': prediction == row['label']\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                if prediction == 'error':\n",
    "                    errors.append(result)\n",
    "            \n",
    "            # Progress update\n",
    "            processed = batch_end\n",
    "            if processed % 50 == 0 or processed == len(df):\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = processed / elapsed if elapsed > 0 else 0\n",
    "                remaining = (len(df) - processed) / rate if rate > 0 else 0\n",
    "                \n",
    "                print(f\"  Progress: {processed:,}/{len(df):,} \"\n",
    "                      f\"({processed/len(df)*100:.1f}%) | \"\n",
    "                      f\"Rate: {rate:.1f} emails/s | \"\n",
    "                      f\"ETA: {remaining/60:.1f}m | \"\n",
    "                      f\"Errors: {len(errors)}\", \n",
    "                      end='\\r')\n",
    "            \n",
    "            # Checkpoint\n",
    "            if checkpoint_every and processed % checkpoint_every < batch_size:\n",
    "                self._save_checkpoint(results, dataset_name, processed)\n",
    "        \n",
    "        print()  # New line after progress bar\n",
    "        elapsed = time.time() - start_time\n",
    "        metrics = self._calculate_metrics(results, elapsed)\n",
    "        self._print_summary(dataset_name, metrics, len(errors))\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'metrics': metrics,\n",
    "            'errors': errors if CONFIG['save_errors'] else []\n",
    "        }\n",
    "    \n",
    "    def _calculate_metrics(self, results: List[Dict], elapsed: float) -> Dict:\n",
    "        valid_results = [r for r in results if r['prediction'] != 'error']\n",
    "        \n",
    "        if not valid_results:\n",
    "            return {\n",
    "                'accuracy': 0.0,\n",
    "                'correct': 0,\n",
    "                'total': 0,\n",
    "                'error_rate': 1.0,\n",
    "                'time_seconds': elapsed\n",
    "            }\n",
    "        \n",
    "        correct = sum(1 for r in valid_results if r['correct'])\n",
    "        total = len(valid_results)\n",
    "        error_count = len(results) - total\n",
    "        \n",
    "        tp = sum(1 for r in valid_results if r['true_label'] == 'phishing_email' and r['prediction'] == 'phishing_email')\n",
    "        fp = sum(1 for r in valid_results if r['true_label'] == 'safe_email' and r['prediction'] == 'phishing_email')\n",
    "        tn = sum(1 for r in valid_results if r['true_label'] == 'safe_email' and r['prediction'] == 'safe_email')\n",
    "        fn = sum(1 for r in valid_results if r['true_label'] == 'phishing_email' and r['prediction'] == 'safe_email')\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': correct / total,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'correct': correct,\n",
    "            'total': total,\n",
    "            'errors': error_count,\n",
    "            'error_rate': error_count / len(results),\n",
    "            'time_seconds': elapsed,\n",
    "            'emails_per_second': len(results) / elapsed if elapsed > 0 else 0,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': tp,\n",
    "                'false_positives': fp,\n",
    "                'true_negatives': tn,\n",
    "                'false_negatives': fn\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _print_summary(self, dataset_name: str, metrics: Dict, error_count: int):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"RESULTS: {dataset_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Accuracy:   {metrics['accuracy']:.2%} ({metrics['correct']:,}/{metrics['total']:,})\")\n",
    "        print(f\"Precision:  {metrics['precision']:.2%}\")\n",
    "        print(f\"Recall:     {metrics['recall']:.2%}\")\n",
    "        print(f\"F1-Score:   {metrics['f1_score']:.2%}\")\n",
    "        print(f\"Errors:     {error_count:,} ({metrics['error_rate']:.1%})\")\n",
    "        print(f\"Time:       {metrics['time_seconds']/60:.1f} minutes\")\n",
    "        print(f\"Speed:      {metrics['emails_per_second']:.1f} emails/second\")\n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        cm = metrics['confusion_matrix']\n",
    "        print(f\"  TP: {cm['true_positives']:,}  |  FP: {cm['false_positives']:,}\")\n",
    "        print(f\"  FN: {cm['false_negatives']:,}  |  TN: {cm['true_negatives']:,}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    def _save_checkpoint(self, results: List[Dict], dataset_name: str, count: int):\n",
    "        checkpoint_file = self.checkpoint_dir / f\"checkpoint_{dataset_name}_{count}.json\"\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(results, f)\n",
    "        print(f\"\\n  ✓ Checkpoint saved: {checkpoint_file.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():    \n",
    "    # Setup GPU\n",
    "    setup_kaggle_gpu()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHISHING DETECTION - ZERO-SHOT BASELINE (FIXED)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {CONFIG['model_name']}\")\n",
    "    print(f\"Strategy: {CONFIG['strategy']}\")\n",
    "    print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load model\n",
    "    generator, tokenizer = ModelLoader.load(\n",
    "        CONFIG['model_name'],\n",
    "        use_4bit=CONFIG['use_4bit']\n",
    "    )\n",
    "    \n",
    "    # Create detector\n",
    "    detector = PhishingDetector(generator, tokenizer)\n",
    "    evaluator = Evaluator(detector)\n",
    "    \n",
    "    # Store results\n",
    "    all_results = {\n",
    "        'model': CONFIG['model_name'],\n",
    "        'config': CONFIG,\n",
    "        'timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'datasets': {}\n",
    "    }\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset_name, filepath in CONFIG['datasets'].items():\n",
    "        try:\n",
    "            df = load_dataset(\n",
    "                filepath,\n",
    "                sample_size=CONFIG['sample_sizes'][dataset_name],\n",
    "                balanced=CONFIG['balanced_sampling']\n",
    "            )\n",
    "            \n",
    "            dataset_results = evaluator.evaluate(\n",
    "                df, \n",
    "                dataset_name,\n",
    "                checkpoint_every=CONFIG['checkpoint_every'],\n",
    "                batch_size=CONFIG['batch_size']\n",
    "            )\n",
    "            \n",
    "            all_results['datasets'][dataset_name] = dataset_results\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n⚠️  Interrupted by user!\")\n",
    "            print(\"Saving partial results...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error processing {dataset_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save results\n",
    "    model_short_name = CONFIG['model_name'].split('/')[-1]\n",
    "    output_file = f\"results_{model_short_name}_{CONFIG['strategy']}.json\"\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {CONFIG['model_name']}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for dataset_name, data in all_results['datasets'].items():\n",
    "        m = data['metrics']\n",
    "        print(f\"{dataset_name:10s} | \"\n",
    "              f\"Acc: {m['accuracy']:6.1%} | \"\n",
    "              f\"F1: {m['f1_score']:6.1%} | \"\n",
    "              f\"Time: {m['time_seconds']/60:5.1f}m | \"\n",
    "              f\"Speed: {m['emails_per_second']:4.1f}/s\")\n",
    "    \n",
    "    print(f\"\\n✓ Results saved: {output_file}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
