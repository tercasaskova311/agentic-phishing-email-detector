{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI AGENT - GRAPH\n",
    "\n",
    "\n",
    "This notebook is designed to be run in **Kaggle notebooks** to ensure reproducibility of the experiment.\n",
    "\n",
    "Environment Setup (Required)\n",
    "\n",
    "1. Set up `HF_TOKEN` as a Kaggle secret  \n",
    "2. Enable internet access  \n",
    "3. Select a GPU runtime\n",
    "4. Upload datasets and update paths in the code\n",
    "\n",
    "Options for testing graph:\n",
    "\n",
    "    'models': {\n",
    "        'email_agent': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "        'url_agent': 'google/gemma-2-2b-it',\n",
    "        'pattern_agent': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "        'judge_agent': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    },\n",
    "\n",
    "    'models': {\n",
    "        'email_agent': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "        'url_agent': 'google/gemma-2-2b-it',\n",
    "        'pattern_agent': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "        'judge_agent': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "CONFIG = {\n",
    "    'models': {\n",
    "        'email_agent': 'Qwen/Qwen2.5-3B-Instruct',\n",
    "        'url_agent': 'google/gemma-2-2b-it',\n",
    "        'pattern_agent': 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "        'judge_agent': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    },\n",
    "    \n",
    "    'datasets': {\n",
    "        'aigen': '/kaggle/input/phishing-emails/aigen.csv',\n",
    "        'enron': '/kaggle/input/phishing-emails/enron.csv',\n",
    "        'trec': '/kaggle/input/phishing-emails/trec.csv'\n",
    "    },\n",
    "    \n",
    "    'sample_sizes': {\n",
    "        'aigen': None,\n",
    "        'enron': 3000,\n",
    "        'trec': 3000\n",
    "    },\n",
    "    \n",
    "    'balanced_sampling': True,\n",
    "    'save_errors': True,\n",
    "    'checkpoint_every': 100,\n",
    "    'batch_size': 10,\n",
    "    'experiment_name': 'multi_agent_crewai',\n",
    "}\n",
    "\n",
    "#helper fucntion =============================\n",
    "def email_analysis_tool(email_text: str) -> dict:\n",
    "        \"\"\"\n",
    "    Analyze email content for phishing cues.\n",
    "        \n",
    "        Returns a dictionary of indicators:\n",
    "        - suspicious_words: list of keywords like 'urgent', 'verify', etc.\n",
    "        - excessive_punctuation: count of '!!' or more\n",
    "        - all_caps: count of words in all caps\n",
    "        \"\"\"\n",
    "    indicators = {}\n",
    "\n",
    "        # Suspicious keywords\n",
    "    suspicious_words_list = [\n",
    "        \"urgent\", \"verify\", \"password\", \"login\", \"account suspended\", \"click here\", \"confirm\"\n",
    "    ]\n",
    "    indicators['suspicious_words'] = [\n",
    "        w for w in suspicious_words_list if w in email_text.lower()\n",
    "    ]\n",
    "\n",
    "        # Excessive punctuation\n",
    "    indicators['excessive_punctuation'] = len(re.findall(r\"[!]{2,}\", email_text))\n",
    "\n",
    "        # All caps words\n",
    "    indicators['all_caps'] = len(re.findall(r\"\\b[A-Z]{4,}\\b\", email_text))\n",
    "\n",
    "    return indicators\n",
    "\n",
    "\n",
    "def url_extraction_tool(email_text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Extract URLs from email and check for suspicious patterns.\n",
    "        \n",
    "        Returns a dictionary with:\n",
    "        - urls: list of dictionaries for each URL\n",
    "        - url: the URL itself\n",
    "        - is_ip: True if URL uses IP instead of domain\n",
    "        - suspicious_length: True if URL length > 75 characters\n",
    "        - odd_subdomains: True if URL has >3 subdomain levels\n",
    "        - total_urls: total number of URLs found\n",
    "        \"\"\"\n",
    "    urls = re.findall(r'(https?://[^\\s]+)', email_text)\n",
    "    url_indicators = []\n",
    "\n",
    "    for url in urls:\n",
    "        parsed = urlparse(url)\n",
    "        hostname = parsed.hostname or \"\"\n",
    "        indicator = {\n",
    "            \"url\": url,\n",
    "            \"is_ip\": all(c.isdigit() or c == '.' for c in hostname),\n",
    "            \"suspicious_length\": len(url) > 75,\n",
    "            \"odd_subdomains\": hostname.count(\".\") > 3\n",
    "        }\n",
    "        url_indicators.append(indicator)\n",
    "\n",
    "    return {\n",
    "        \"urls\": url_indicators,\n",
    "        \"total_urls\": len(urls)\n",
    "    }\n",
    "\n",
    "def get_llm_from_hf(model_name: str):\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if not hf_token:\n",
    "        raise RuntimeError(\"HF_TOKEN not set\")\n",
    "\n",
    "    return HuggingFaceEndpoint(\n",
    "        repo_id=model_name,\n",
    "        huggingfacehub_api_token=hf_token,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=512,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        timeout=120,\n",
    "    )\n",
    "\n",
    "# AGENTS SETUP\n",
    "# ============================================================================\n",
    "\n",
    "def create_agents():\n",
    "    \"\"\"Create all agents with specified models.\"\"\"\n",
    "    email_agent = Agent(\n",
    "        role=\"Email Content Analyst\",\n",
    "        goal=\"Detect phishing language in emails - context, tricky and spam language\",\n",
    "        backstory=\"Cybersecurity analyst specializing in linguistic phishing cues.\",\n",
    "        llm=get_llm_from_hf(CONFIG['models']['email_agent']),\n",
    "        tools=[Tools.email_analysis_tool],\n",
    "        allow_delegation=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    url_agent = Agent(\n",
    "        role=\"URL Inspector\",\n",
    "        goal=\"Analyze URLs for phishing indicators\",\n",
    "        backstory=\"Expert in malicious URLs and domain obfuscation.\",\n",
    "        llm=get_llm_from_hf(CONFIG['models']['url_agent']),\n",
    "        tools=[Tools.url_extraction_tool],\n",
    "        allow_delegation=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    pattern_agent = Agent(\n",
    "        role=\"Email Pattern Analyst\",\n",
    "        goal=\"Detect known phishing templates and metadata inconsistencies\",\n",
    "        backstory=\"Specialist in email headers, subject, sender spoofing, and scam patterns.\",\n",
    "        llm=get_llm_from_hf(CONFIG['models']['pattern_agent']),\n",
    "        allow_delegation=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    judge_agent = Agent(\n",
    "        role=\"Final Judge\",\n",
    "        goal=\"Aggregate all analyses and decide if the email is phishing\",\n",
    "        backstory=\"Senior security expert making final classification decisions.\",\n",
    "        llm=get_llm_from_hf(CONFIG['models']['judge_agent']),\n",
    "        allow_delegation=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    return email_agent, url_agent, pattern_agent, judge_agent\n",
    "\n",
    "# TASK BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "def build_tasks(email_text: str, agents: Tuple):\n",
    "    \"\"\"Build task graph for a single email.\"\"\"\n",
    "    email_agent, url_agent, pattern_agent, judge_agent = agents\n",
    "    \n",
    "    t1 = Task(\n",
    "        description=f\"\"\"\n",
    "Analyze the email text for phishing language.\n",
    "Email:\n",
    "{email_text[:800]}\n",
    "\n",
    "Provide: Suspicious language indicators\n",
    "\"\"\",\n",
    "        agent=email_agent,\n",
    "        expected_output=\"Suspicious language indicators\",\n",
    "    )\n",
    "    \n",
    "    t2 = Task(\n",
    "        description=f\"\"\"\n",
    "Extract and analyze URLs in the email.\n",
    "Email:\n",
    "{email_text[:800]}\n",
    "\n",
    "Provide: URL-based phishing indicators\n",
    "\"\"\",\n",
    "        agent=url_agent,\n",
    "        expected_output=\"URL-based phishing indicators\",\n",
    "    )\n",
    "    \n",
    "    t3 = Task(\n",
    "        description=f\"\"\"\n",
    "Analyze sender, email subject formatting, and known phishing patterns.\n",
    "Email:\n",
    "{email_text[:800]}\n",
    "\n",
    "Provide: Pattern-based phishing indicators\n",
    "\"\"\",\n",
    "        agent=pattern_agent,\n",
    "        expected_output=\"Pattern-based phishing indicators\",\n",
    "    )\n",
    "    \n",
    "    t4 = Task(\n",
    "        description=\"\"\"\n",
    "You are given three analyses:\n",
    "1) Email language analysis\n",
    "2) URL analysis\n",
    "3) Pattern analysis\n",
    "\n",
    "Combine them and respond EXACTLY in this format:\n",
    "DECISION: PHISHING or SAFE\n",
    "CONFIDENCE: integer 0-100\n",
    "REASON: one sentence\n",
    "\n",
    "Your response:\n",
    "\"\"\",\n",
    "        agent=judge_agent,\n",
    "        context=[t1, t2, t3],\n",
    "        expected_output=\"Final phishing verdict\",\n",
    "    )\n",
    "    \n",
    "    return [t1, t2, t3, t4]\n",
    "\n",
    "# RESPONSE PARSER\n",
    "# ============================================================================\n",
    "\n",
    "def parse_response(text: str) -> Tuple[str, float, str]:\n",
    "    \"\"\"Parse agent response into structured format.\"\"\"\n",
    "    decision_match = re.search(r\"DECISION:\\s*(PHISHING|SAFE)\", text, re.IGNORECASE)\n",
    "    conf_match = re.search(r\"CONFIDENCE:\\s*(\\d+)\", text)\n",
    "    \n",
    "    decision = decision_match.group(1).upper() if decision_match else \"SAFE\"\n",
    "    confidence = int(conf_match.group(1)) / 100.0 if conf_match else 0.5\n",
    "    \n",
    "    # Map to expected labels\n",
    "    prediction = \"phishing_email\" if decision == \"PHISHING\" else \"safe_email\"\n",
    "    \n",
    "    return prediction, confidence, text.strip()[:200]\n",
    "\n",
    "# ============================================================================\n",
    "# MULTI-AGENT DETECTOR\n",
    "# ============================================================================\n",
    "\n",
    "class MultiAgentDetector:\n",
    "    \"\"\"Multi-agent phishing detector with CrewAI.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Initializing multi-agent system...\")\n",
    "        self.agents = create_agents()\n",
    "        print(\"✓ All agents created\")\n",
    "    \n",
    "    def analyze_email(self, email_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze a single email with multi-agent crew.\"\"\"\n",
    "        try:\n",
    "            tasks = build_tasks(email_text, self.agents)\n",
    "            crew = Crew(\n",
    "                agents=list(self.agents),\n",
    "                tasks=tasks,\n",
    "                process=Process.sequential,\n",
    "                verbose=False,\n",
    "            )\n",
    "            \n",
    "            result = crew.kickoff()\n",
    "            prediction, confidence, reasoning = parse_response(str(result))\n",
    "            \n",
    "            return {\n",
    "                'prediction': prediction,\n",
    "                'confidence': confidence,\n",
    "                'reasoning': reasoning\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'prediction': 'error',\n",
    "                'confidence': 0.5,\n",
    "                'reasoning': f'Error: {str(e)[:100]}'\n",
    "            }\n",
    "    \n",
    "    def analyze_batch(self, emails: List[str], show_progress: bool = True) -> List[Dict]:\n",
    "        \"\"\"Process batch of emails.\"\"\"\n",
    "        results = []\n",
    "        iterator = tqdm(emails, desc=\"Processing\") if show_progress else emails\n",
    "        \n",
    "        for email in iterator:\n",
    "            result = self.analyze_email(email)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# DATA LOADING (same as single-agent)\n",
    "# ============================================================================\n",
    "\n",
    "def load_dataset(\n",
    "    filepath: str,\n",
    "    sample_size: int = None,\n",
    "    balanced: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Load and optionally sample dataset.\"\"\"\n",
    "    print(f\"\\nLoading: {filepath}\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"  Original size: {len(df):,}\")\n",
    "    \n",
    "    class_counts = df['label'].value_counts()\n",
    "    for label, count in class_counts.items():\n",
    "        print(f\"    {label}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if sample_size and sample_size < len(df):\n",
    "        if balanced:\n",
    "            n_per_class = sample_size // len(class_counts)\n",
    "            df = df.groupby('label', group_keys=False).apply(\n",
    "                lambda x: x.sample(min(len(x), n_per_class), random_state=42)\n",
    "            ).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  Sampled size: {len(df):,}\")\n",
    "        sampled_counts = df['label'].value_counts()\n",
    "        print(f\"  Sampled distribution:\")\n",
    "        for label, count in sampled_counts.items():\n",
    "            print(f\"    {label}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# EVALUATOR\n",
    "# ============================================================================\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"Evaluate multi-agent detector on datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, detector: MultiAgentDetector, checkpoint_dir: Path = Path('.')):\n",
    "        self.detector = detector\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "    \n",
    "    def evaluate(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        dataset_name: str,\n",
    "        checkpoint_every: int = 100,\n",
    "        batch_size: int = 10\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate detector on dataset with checkpointing.\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EVALUATING: {dataset_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total emails: {len(df):,}\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        \n",
    "        results = []\n",
    "        errors = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, len(df), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(df))\n",
    "            batch_df = df.iloc[batch_start:batch_end]\n",
    "            batch_emails = batch_df['message'].tolist()\n",
    "            \n",
    "            # Analyze batch\n",
    "            batch_results = self.detector.analyze_batch(batch_emails, show_progress=False)\n",
    "            \n",
    "            # Store results\n",
    "            for idx, result in enumerate(batch_results):\n",
    "                actual_idx = batch_start + idx\n",
    "                row = batch_df.iloc[idx]\n",
    "                \n",
    "                full_result = {\n",
    "                    'email_id': actual_idx,\n",
    "                    'true_label': row['label'],\n",
    "                    'prediction': result['prediction'],\n",
    "                    'confidence': result['confidence'],\n",
    "                    'reasoning': result['reasoning'],\n",
    "                    'correct': result['prediction'] == row['label']\n",
    "                }\n",
    "                results.append(full_result)\n",
    "                \n",
    "                if result['prediction'] == 'error':\n",
    "                    errors.append(full_result)\n",
    "            \n",
    "            # Progress update\n",
    "            processed = batch_end\n",
    "            if processed % 10 == 0 or processed == len(df):\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = processed / elapsed if elapsed > 0 else 0\n",
    "                remaining = (len(df) - processed) / rate if rate > 0 else 0\n",
    "                \n",
    "                print(f\"  Progress: {processed:,}/{len(df):,} \"\n",
    "                      f\"({processed/len(df)*100:.1f}%) | \"\n",
    "                      f\"Rate: {rate:.1f} emails/s | \"\n",
    "                      f\"ETA: {remaining/60:.1f}m | \"\n",
    "                      f\"Errors: {len(errors)}\", \n",
    "                      end='\\r')\n",
    "            \n",
    "            # Checkpoint\n",
    "            if checkpoint_every and processed % checkpoint_every < batch_size:\n",
    "                self._save_checkpoint(results, dataset_name, processed)\n",
    "        \n",
    "        print()  # New line after progress\n",
    "        elapsed = time.time() - start_time\n",
    "        metrics = self._calculate_metrics(results, elapsed)\n",
    "        self._print_summary(dataset_name, metrics, len(errors))\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'metrics': metrics,\n",
    "            'errors': errors if CONFIG['save_errors'] else []\n",
    "        }\n",
    "    \n",
    "    def _calculate_metrics(self, results: List[Dict], elapsed: float) -> Dict:\n",
    "        \"\"\"Calculate comprehensive metrics.\"\"\"\n",
    "        valid_results = [r for r in results if r['prediction'] != 'error']\n",
    "        \n",
    "        if not valid_results:\n",
    "            return {\n",
    "                'accuracy': 0.0,\n",
    "                'correct': 0,\n",
    "                'total': 0,\n",
    "                'error_rate': 1.0,\n",
    "                'time_seconds': elapsed\n",
    "            }\n",
    "        \n",
    "        correct = sum(1 for r in valid_results if r['correct'])\n",
    "        total = len(valid_results)\n",
    "        error_count = len(results) - total\n",
    "        \n",
    "        tp = sum(1 for r in valid_results if r['true_label'] == 'phishing_email' and r['prediction'] == 'phishing_email')\n",
    "        fp = sum(1 for r in valid_results if r['true_label'] == 'safe_email' and r['prediction'] == 'phishing_email')\n",
    "        tn = sum(1 for r in valid_results if r['true_label'] == 'safe_email' and r['prediction'] == 'safe_email')\n",
    "        fn = sum(1 for r in valid_results if r['true_label'] == 'phishing_email' and r['prediction'] == 'safe_email')\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': correct / total,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'correct': correct,\n",
    "            'total': total,\n",
    "            'errors': error_count,\n",
    "            'error_rate': error_count / len(results),\n",
    "            'time_seconds': elapsed,\n",
    "            'emails_per_second': len(results) / elapsed if elapsed > 0 else 0,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': tp,\n",
    "                'false_positives': fp,\n",
    "                'true_negatives': tn,\n",
    "                'false_negatives': fn\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _print_summary(self, dataset_name: str, metrics: Dict, error_count: int):\n",
    "        \"\"\"Print evaluation summary.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"RESULTS: {dataset_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Accuracy:   {metrics['accuracy']:.2%} ({metrics['correct']:,}/{metrics['total']:,})\")\n",
    "        print(f\"Precision:  {metrics['precision']:.2%}\")\n",
    "        print(f\"Recall:     {metrics['recall']:.2%}\")\n",
    "        print(f\"F1-Score:   {metrics['f1_score']:.2%}\")\n",
    "        print(f\"Errors:     {error_count:,} ({metrics['error_rate']:.1%})\")\n",
    "        print(f\"Time:       {metrics['time_seconds']/60:.1f} minutes\")\n",
    "        print(f\"Speed:      {metrics['emails_per_second']:.1f} emails/second\")\n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        cm = metrics['confusion_matrix']\n",
    "        print(f\"  TP: {cm['true_positives']:,}  |  FP: {cm['false_positives']:,}\")\n",
    "        print(f\"  FN: {cm['false_negatives']:,}  |  TN: {cm['true_negatives']:,}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    def _save_checkpoint(self, results: List[Dict], dataset_name: str, count: int):\n",
    "        \"\"\"Save checkpoint.\"\"\"\n",
    "        checkpoint_file = self.checkpoint_dir / f\"checkpoint_{dataset_name}_{count}.json\"\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(results, f)\n",
    "        print(f\"\\n  ✓ Checkpoint saved: {checkpoint_file.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_metrics(all_results: Dict):\n",
    "    \"\"\"Plot comparison across datasets.\"\"\"\n",
    "    datasets = []\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for dataset_name, data in all_results['datasets'].items():\n",
    "        datasets.append(dataset_name)\n",
    "        accuracies.append(data['metrics']['accuracy'])\n",
    "        f1_scores.append(data['metrics']['f1_score'])\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "    ax.bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Dataset')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Multi-Agent Performance Across Datasets')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('multi_agent_performance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# MAIN EXECUTION\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-AGENT PHISHING DETECTION - CrewAI\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Email Agent:   {CONFIG['models']['email_agent']}\")\n",
    "    print(f\"URL Agent:     {CONFIG['models']['url_agent']}\")\n",
    "    print(f\"Pattern Agent: {CONFIG['models']['pattern_agent']}\")\n",
    "    print(f\"Judge Agent:   {CONFIG['models']['judge_agent']}\")\n",
    "    print(f\"Batch size:    {CONFIG['batch_size']}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create detector\n",
    "    detector = MultiAgentDetector()\n",
    "    evaluator = Evaluator(detector)\n",
    "    \n",
    "    # Store results\n",
    "    all_results = {\n",
    "        'models': CONFIG['models'],\n",
    "        'config': CONFIG,\n",
    "        'timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'datasets': {}\n",
    "    }\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset_name, filepath in CONFIG['datasets'].items():\n",
    "        try:\n",
    "            df = load_dataset(\n",
    "                filepath,\n",
    "                sample_size=CONFIG['sample_sizes'][dataset_name],\n",
    "                balanced=CONFIG['balanced_sampling']\n",
    "            )\n",
    "            \n",
    "            dataset_results = evaluator.evaluate(\n",
    "                df,\n",
    "                dataset_name,\n",
    "                checkpoint_every=CONFIG['checkpoint_every'],\n",
    "                batch_size=CONFIG['batch_size']\n",
    "            )\n",
    "            \n",
    "            all_results['datasets'][dataset_name] = dataset_results\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n⚠️  Interrupted by user!\")\n",
    "            print(\"Saving partial results...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error processing {dataset_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Save results\n",
    "    output_file = f\"results_multiagent_{CONFIG['experiment_name']}.json\"\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Multi-Agent System Performance\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for dataset_name, data in all_results['datasets'].items():\n",
    "        m = data['metrics']\n",
    "        print(f\"{dataset_name:10s} | \"\n",
    "              f\"Acc: {m['accuracy']:6.1%} | \"\n",
    "              f\"F1: {m['f1_score']:6.1%} | \"\n",
    "              f\"Time: {m['time_seconds']/60:5.1f}m | \"\n",
    "              f\"Speed: {m['emails_per_second']:4.1f}/s\")\n",
    "    \n",
    "    print(f\"\\n✓ Results saved: {output_file}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Plot results\n",
    "    try:\n",
    "        plot_metrics(all_results)\n",
    "    except:\n",
    "        print(\"⚠️  Could not generate plots\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
